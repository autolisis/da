{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target' 'id' 'date' 'flag' 'user' 'text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tweets.csv', encoding='latin1')\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = df.text\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))\n",
    "print(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nlp\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "text = []\n",
    "for each in data.text:\n",
    "    each = re.sub(\"[^a-zA-Z]\", \" \", str(each))\n",
    "    # lowercase all\n",
    "    each = each.lower()\n",
    "    # split all by tokenizing\n",
    "    each = nlp.word_tokenize(each)\n",
    "    # delete stop words from your array\n",
    "    each = [word for word in each if not word in set(\n",
    "        stopwords.words(\"english\"))]\n",
    "    # lemmatize \"memories\" -> \"memory\"\n",
    "    each = [lemma.lemmatize(word) for word in each]\n",
    "#     # put them into big array\n",
    "    text.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('tweets-lemma.txt', 'wb') as f:\n",
    "#     pickle.dump(text, f)\n",
    "# with open('tweets-lemma.txt', 'rb') as f:\n",
    "    # text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "model = Word2Vec(text, min_count=2)\n",
    "# model.wv.save_word2vec_format('word2vec-tweets')\n",
    "# model = KeyedVectors.load_word2vec_format('word2vec-tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sister', 0.841433048248291), ('cousin', 0.7108091115951538), ('bro', 0.6551007628440857), ('bros', 0.6453346610069275), ('dad', 0.6419702768325806), ('nephew', 0.637410581111908), ('maggiex', 0.6359193921089172), ('brothes', 0.6213483214378357), ('limor', 0.6047430634498596), ('victoriax', 0.5994175672531128)]\n"
     ]
    }
   ],
   "source": [
    "print(model.similar_by_word('brother'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 100\n"
     ]
    }
   ],
   "source": [
    "# Take the 15000 most common words\n",
    "word_vectors = model.syn0[:15000]\n",
    "n_words = word_vectors.shape[0]\n",
    "vector_size = word_vectors.shape[1]\n",
    "print(n_words, vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.15, min_samples=2, n_jobs=4, metric='cosine')\n",
    "idx = dbscan.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters:  429\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Number of clusters: ', len(np.unique(idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_list = list(zip(model.index2word, idx))\n",
    "word_centroid_list_sort = sorted(word_centroid_list, key=lambda el: el[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = {}\n",
    "file_out = open('cluster-output', \"w\")\n",
    "file_out.write(\"WORD\\tCLUSTER_ID\\n\")\n",
    "for word_centroid in word_centroid_list_sort:\n",
    "    line = word_centroid[0] + '\\t' + str(word_centroid[1]) + '\\n'\n",
    "    cluster[word_centroid[0]] = word_centroid[1]\n",
    "    file_out.write(line)\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 4\ntea 4\nwater -1\ngood 1\nbad -1\nbeautiful 51\nugly -1\npen -1\nchocolate 4\nfood -1\npizza 4\nhungry 83\ncompany -1\nwater -1\nrain 4\nsun 4\nhurricane -1\nbrother 4\nsister 4\nfather 67\nniece 4\nson 109\nschool 21\ncollege 21\nuniversity 4\nenglish 4\nfrench -1\ngerman 4\nhindi 4\ntamil 4\n"
     ]
    }
   ],
   "source": [
    "# See if there's any semantic meaning captured in the clustering\n",
    "words = ['coffee', 'tea', 'water','good', 'bad',\n",
    "         'beautiful', 'ugly', 'pen', 'chocolate',\n",
    "         'food', 'pizza', 'hungry', 'company', 'water', 'rain', 'sun',\n",
    "         'hurricane', 'brother', 'sister', 'father', 'niece', 'son',\n",
    "         'school', 'college', 'university',\n",
    "         'english', 'french', 'german',\n",
    "         'hindi', 'tamil',\n",
    "         ]\n",
    "for word in words:\n",
    "    print(word, cluster[word])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
